import requests
import re
import json
import html
import socket
import time
import jdatetime
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# Ù„ÛŒØ³Øª Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ
CHANNELS = [
    "Myporoxy", "TelMTProto", "ProxyMTProto", "mt_p_roxy", 
    "ProxyHagh", "MTProtoProxies", "PinkProxy", "v2rayng_vpn"
]

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ
CHECK_INTERVAL = 30  # Ù‡Ø± Ú†Ù†Ø¯ Ø«Ø§Ù†ÛŒÙ‡ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø±Ø§ Ú†Ú© Ú©Ù†Ø¯ (Ø¯Ø± Ø²Ù…Ø§Ù† Ù‚Ø·Ø¹ÛŒ)
RUN_INTERVAL = 600   # Ù‡Ø± Ú†Ù†Ø¯ Ø«Ø§Ù†ÛŒÙ‡ (Û±Û° Ø¯Ù‚ÛŒÙ‚Ù‡) Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯ (Ø¯Ø± Ø²Ù…Ø§Ù† ÙˆØµÙ„ÛŒ)

def check_internet():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø¨Ø§ Ù¾ÛŒÙ†Ú¯ Ú©Ø±Ø¯Ù† DNS Ú¯ÙˆÚ¯Ù„"""
    try:
        # Ø§ØªØµØ§Ù„ Ø¨Ù‡ 8.8.8.8 Ù¾ÙˆØ±Øª 53 (DNS) Ø¨Ø§ ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª 3 Ø«Ø§Ù†ÛŒÙ‡
        socket.create_connection(("8.8.8.8", 53), timeout=3)
        return True
    except OSError:
        return False

def get_ping(server, port):
    """ØªØ³Øª Ù¾ÛŒÙ†Ú¯ ÙˆØ§Ù‚Ø¹ÛŒ (Ø§ØªØµØ§Ù„ TCP)"""
    try:
        start = time.time()
        sock = socket.create_connection((server, int(port)), timeout=1.5)
        sock.close()
        return int((time.time() - start) * 1000)
    except:
        return 9999

def process_proxy(link):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ ØªØ³Øª Ù¾ÛŒÙ†Ú¯ Ù‡Ø± Ù„ÛŒÙ†Ú©"""
    try:
        if "https://t.me/proxy" in link:
            link = link.replace("https://t.me/proxy", "tg://proxy")
            
        server_match = re.search(r'server=([\w\.\-\[\]:]+)', link)
        port_match = re.search(r'port=(\d+)', link)
        secret_match = re.search(r'secret=([\w\.\-\%]+)', link)

        if not (server_match and port_match and secret_match):
            return None

        server = server_match.group(1)
        port = port_match.group(1)
        secret = secret_match.group(1)
        
        ping = get_ping(server, port)
        
        if ping < 3000: 
            return {
                "server": server,
                "port": port,
                "secret": secret,
                "link": link,
                "ping": ping
            }
    except Exception as e:
        return None
    return None

def run_scraper():
    """Ø¨Ø¯Ù†Ù‡ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ú©Ù‡ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯"""
    print(f"\nğŸš€ Ø´Ø±ÙˆØ¹ Ø¹Ù…Ù„ÛŒØ§Øª Ø¯Ø± Ø³Ø§Ø¹Øª: {datetime.now().strftime('%H:%M:%S')}")
    print("ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§...")
    
    raw_links = []
    for chan in CHANNELS:
        try:
            url = f"https://t.me/s/{chan}"
            # ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ø±Ø§ Ú©Ù…ÛŒ Ø¨ÛŒØ´ØªØ± Ú©Ø±Ø¯ÛŒÙ… ØªØ§ Ø¯Ø± Ø§ÛŒÙ†ØªØ±Ù†Øª Ø¶Ø¹ÛŒÙ Ù‡Ù… Ú©Ø§Ø± Ú©Ù†Ø¯
            res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=15)
            clean_text = html.unescape(res.text)
            links = re.findall(r'(?:https://t\.me/|tg://)proxy\?(?:[^"\s>]+)', clean_text)
            raw_links.extend(links)
            print(f"âœ… {chan}: {len(links)} ÛŒØ§ÙØª Ø´Ø¯.")
        except requests.exceptions.RequestException as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª {chan}: {e}")
            continue
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø³ Ø¯Ø± {chan}")
            continue

    unique_links = list(set(raw_links))
    valid_proxies = []

    if not unique_links:
        print("âš ï¸ Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú©ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§ÛŒÙ†ØªØ±Ù†Øª Ú†Ú© Ø´ÙˆØ¯.")
        return

    print(f"âš¡ Ø¯Ø± Ø­Ø§Ù„ ØªØ³Øª Ù¾ÛŒÙ†Ú¯ {len(unique_links)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ...")
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ThreadPoolExecutor Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§
    with ThreadPoolExecutor(max_workers=50) as executor:
        results = executor.map(process_proxy, unique_links)
        for r in results:
            if r: valid_proxies.append(r)

    valid_proxies.sort(key=lambda x: x['ping'])

    # --- ØªÙ†Ø¸ÛŒÙ… Ø²Ù…Ø§Ù† ---
    utc_now = datetime.utcnow()
    tehran_time = utc_now + timedelta(hours=3, minutes=30)
    now_shamsi = jdatetime.datetime.fromgregorian(datetime=tehran_time).strftime("%Y/%m/%d - %H:%M")
    
    final_output = {
        "last_updated": now_shamsi,
        "proxies": valid_proxies
    }

    try:
        with open("proxies.json", "w", encoding="utf-8") as f:
            json.dump(final_output, f, indent=4, ensure_ascii=False)
        print(f"ğŸ’¾ ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø±ÙˆÚ©Ø³ÛŒ: {len(valid_proxies)}")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„: {e}")

    print(f"ğŸ Ù¾Ø§ÛŒØ§Ù† Ø¹Ù…Ù„ÛŒØ§Øª. Ø²Ù…Ø§Ù†: {now_shamsi}")

if __name__ == "__main__":
    print("âœ… Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¶Ø¯ Ù‚Ø·Ø¹ÛŒ Ø§ÛŒÙ†ØªØ±Ù†Øª ÙØ¹Ø§Ù„ Ø´Ø¯.")
    print("--------------------------------------")
    
    while True:
        try:
            if check_internet():
                # Ø§Ú¯Ø± Ø§ÛŒÙ†ØªØ±Ù†Øª ÙˆØµÙ„ Ø¨ÙˆØ¯ØŒ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯
                run_scraper()
                
                print(f"ğŸ’¤ Ø§Ø³ØªØ±Ø§Ø­Øª Ø¨Ø±Ø§ÛŒ {RUN_INTERVAL} Ø«Ø§Ù†ÛŒÙ‡...")
                time.sleep(RUN_INTERVAL)
            else:
                # Ø§Ú¯Ø± Ø§ÛŒÙ†ØªØ±Ù†Øª Ù‚Ø·Ø¹ Ø¨ÙˆØ¯
                print(f"â›” Ø§ÛŒÙ†ØªØ±Ù†Øª Ù‚Ø·Ø¹ Ø§Ø³Øª! ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø¯Ø± {CHECK_INTERVAL} Ø«Ø§Ù†ÛŒÙ‡ Ø¯ÛŒÚ¯Ø±...")
                time.sleep(CHECK_INTERVAL)
                
        except KeyboardInterrupt:
            print("\nğŸ›‘ Ø¨Ø±Ù†Ø§Ù…Ù‡ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
            break
        except Exception as e:
            # Ø§Ú¯Ø± Ø®Ø·Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ø¨Ø¯ÛŒ Ø±Ø® Ø¯Ø§Ø¯ Ú©Ù‡ Ú©Ù„ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø®ÙˆØ§Ø³Øª Ú©Ø±Ø´ Ú©Ù†Ø¯
            print(f"ğŸ”¥ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ (Critical Error): {e}")
            print("ğŸ”„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¬Ø¯Ø¯ Ø®ÙˆØ¯Ú©Ø§Ø±...")
            time.sleep(10)
