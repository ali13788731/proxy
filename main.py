import requests
import re
import json
import html
import socket
import time
import base64  # Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Base64
import jdatetime
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# Ù„ÛŒØ³Øª Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ
CHANNELS = [
    "Myporoxy", "TelMTProto", "ProxyMTProto", "mt_p_roxy", 
    "ProxyHagh", "MTProtoProxies", "PinkProxy", "v2rayng_vpn"
]

def get_ping(server, port):
    try:
        start = time.time()
        sock = socket.create_connection((server, int(port)), timeout=1.5)
        sock.close()
        return int((time.time() - start) * 1000)
    except:
        return 9999

def process_proxy(link):
    try:
        if "https://t.me/proxy" in link:
            link = link.replace("https://t.me/proxy", "tg://proxy")
            
        server = re.search(r'server=([\w\.\-\[\]:]+)', link).group(1)
        port = re.search(r'port=(\d+)', link).group(1)
        secret = re.search(r'secret=([\w\.\-\%]+)', link).group(1)
        
        ping = get_ping(server, port)
        
        if ping < 3000: 
            return {
                "server": server,
                "port": port,
                "secret": secret,
                "link": link,
                "ping": ping
            }
    except:
        return None

def main():
    print("ğŸš€ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§...")
    raw_links = []
    for chan in CHANNELS:
        try:
            url = f"https://t.me/s/{chan}"
            res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            clean_text = html.unescape(res.text)
            links = re.findall(r'(?:https://t\.me/|tg://)proxy\?(?:[^"\s>]+)', clean_text)
            raw_links.extend(links)
            print(f"âœ… {chan}: {len(links)} ÛŒØ§ÙØª Ø´Ø¯.")
        except:
            continue

    unique_links = list(set(raw_links))
    valid_proxies = []

    print(f"âš¡ Ø¯Ø± Ø­Ø§Ù„ ØªØ³Øª Ù¾ÛŒÙ†Ú¯ {len(unique_links)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ...")
    with ThreadPoolExecutor(max_workers=50) as executor:
        results = executor.map(process_proxy, unique_links)
        for r in results:
            if r: valid_proxies.append(r)

    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾ÛŒÙ†Ú¯
    valid_proxies.sort(key=lambda x: x['ping'])

    # --- Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ ---
    utc_now = datetime.utcnow()
    tehran_time = utc_now + timedelta(hours=3, minutes=30)
    now_shamsi = jdatetime.datetime.fromgregorian(datetime=tehran_time).strftime("%Y/%m/%d - %H:%M")

    # --- Ø®Ø±ÙˆØ¬ÛŒ 1: JSON Ø¨Ø±Ø§ÛŒ Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† Ø§Ù†Ø¯Ø±ÙˆÛŒØ¯ ---
    final_output = {
        "last_updated": now_shamsi,
        "proxies": valid_proxies
    }
    with open("proxies.json", "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    # --- Ø®Ø±ÙˆØ¬ÛŒ 2: Base64 Subscription ---
    # Ø³Ø§Ø®Øª Ù„ÛŒØ³Øª Ù…ØªÙ†ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
    plain_text_links = ""
    for p in valid_proxies:
        plain_text_links += p['link'] + "\n"
    
    # Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ Base64
    base64_bytes = base64.b64encode(plain_text_links.encode('utf-8'))
    base64_string = base64_bytes.decode('utf-8')

    with open("sub.txt", "w", encoding="utf-8") as f:
        f.write(base64_string)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†Ø³Ø®Ù‡ Ù…ØªÙ†ÛŒ Ø³Ø§Ø¯Ù‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
    with open("proxies.txt", "w", encoding="utf-8") as f:
        f.write(plain_text_links)
        
    print(f"ğŸ ØªÙ…Ø§Ù… Ø´Ø¯. {len(valid_proxies)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯. Ø®Ø±ÙˆØ¬ÛŒ Base64 Ø¯Ø± sub.txt Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØª.")

if __name__ == "__main__":
    main()
