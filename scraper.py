import requests
import re
import json
import html
import socket
import time
import jdatetime
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse, parse_qs

CHANNELS = [
    "Myporoxy", "TelMTProto", "ProxyMTProto", "mt_p_roxy", 
    "ProxyHagh", "MTProtoProxies", "PinkProxy", "v2rayng_vpn"
]

def get_ping(server, port):
    try:
        start = time.time()
        sock = socket.create_connection((server, int(port)), timeout=1.5)
        sock.close()
        return int((time.time() - start) * 1000)
    except:
        return 9999

def process_proxy(link):
    try:
        # Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù„ÛŒÙ†Ú©
        clean_link = link.replace("tg://proxy", "https://t.me/proxy")
        parsed_url = urlparse(clean_link)
        params = parse_qs(parsed_url.query)
        
        server = params.get('server', [None])[0]
        port = params.get('port', [None])[0]
        secret = params.get('secret', [None])[0]
        
        if not all([server, port, secret]):
            return None
            
        ping = get_ping(server, port)
        
        if ping < 3000: 
            return {
                "server": server,
                "port": port,
                "secret": secret,
                "link": f"tg://proxy?server={server}&port={port}&secret={secret}",
                "ping": ping
            }
    except Exception as e:
        return None
    return None

def main():
    print("ğŸš€ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§...")
    raw_links = []
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Session Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ± Ùˆ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±
    session = requests.Session()
    session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})

    for chan in CHANNELS:
        try:
            url = f"https://t.me/s/{chan}"
            res = session.get(url, timeout=15)
            # Ø±Ù†Ú©â€ŒÚ¯ÛŒØ±ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
            links = re.findall(r'tg://proxy\?[^"\'\s<>]+|https://t\.me/proxy\?[^"\'\s<>]+', res.text)
            raw_links.extend(links)
            print(f"âœ… {chan}: {len(links)} ÛŒØ§ÙØª Ø´Ø¯.")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ú©Ø§Ù†Ø§Ù„ {chan}: {e}")
            continue

    unique_links = list(set(raw_links))
    valid_proxies = []

    print(f"âš¡ Ø¯Ø± Ø­Ø§Ù„ ØªØ³Øª Ù¾ÛŒÙ†Ú¯ {len(unique_links)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ...")
    with ThreadPoolExecutor(max_workers=30) as executor:
        results = executor.map(process_proxy, unique_links)
        for r in results:
            if r: valid_proxies.append(r)

    valid_proxies.sort(key=lambda x: x['ping'])
    
    # ÙÙ‚Ø· Û³Û° ØªØ§ÛŒ Ø¨Ø±ØªØ± Ø±Ùˆ Ù†Ú¯Ù‡ Ø¯Ø§Ø± (Ø¨Ø±Ø§ÛŒ Ø´Ù„ÙˆØº Ù†Ø´Ø¯Ù† ØµÙØ­Ù‡)
    valid_proxies = valid_proxies[:30]

    now_shamsi = jdatetime.datetime.now().strftime("%Y/%m/%d - %H:%M")
    
    final_output = {
        "last_updated": now_shamsi,
        "proxies": valid_proxies
    }

    with open("proxies.json", "w", encoding="utf-8") as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)
    
    print(f"ğŸ ØªÙ…Ø§Ù… Ø´Ø¯. {len(valid_proxies)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

if __name__ == "__main__":
    main()
