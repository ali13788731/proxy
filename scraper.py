import requests
import re
import json
import html
import socket
import time
import jdatetime
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª ---
CHANNELS = [
    "Myporoxy", "TelMTProto", "ProxyMTProto", "mt_p_roxy", 
    "ProxyHagh", "MTProtoProxies", "PinkProxy", "v2rayng_vpn"
]
CHECK_INTERVAL = 20  # Ø²Ù…Ø§Ù† Ú†Ú© Ù…Ø¬Ø¯Ø¯ Ø¯Ø± ØµÙˆØ±Øª Ù‚Ø·Ø¹ Ø§ÛŒÙ†ØªØ±Ù†Øª (Ø«Ø§Ù†ÛŒÙ‡)
RUN_INTERVAL = 300   # ÙØ§ØµÙ„Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¨ÛŒÙ† Ù‡Ø± Ø¢Ù¾Ø¯ÛŒØª Ú©Ù„ÛŒ (Ø«Ø§Ù†ÛŒÙ‡ - Ûµ Ø¯Ù‚ÛŒÙ‚Ù‡)

def check_internet():
    """Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª"""
    try:
        socket.create_connection(("8.8.8.8", 53), timeout=3)
        return True
    except OSError:
        return False

def get_ping(server, port):
    """ØªØ³Øª Ù¾ÛŒÙ†Ú¯ TCP"""
    try:
        start = time.time()
        sock = socket.create_connection((server, int(port)), timeout=1.5)
        sock.close()
        return int((time.time() - start) * 1000)
    except:
        return 9999

def process_proxy(link):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø´Ø®ØµØ§Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒ"""
    try:
        if "https://t.me/proxy" in link:
            link = link.replace("https://t.me/proxy", "tg://proxy")
        
        server = re.search(r'server=([\w\.\-\[\]:]+)', link).group(1)
        port = re.search(r'port=(\d+)', link).group(1)
        secret = re.search(r'secret=([\w\.\-\%]+)', link).group(1)
        
        ping = get_ping(server, port)
        if ping < 3000:
            return {"server": server, "port": port, "secret": secret, "link": link, "ping": ping}
    except:
        return None

def fetch_channel_links(chan, session):
    """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² Ú©Ø§Ù†Ø§Ù„ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ÙˆØ§Ø²ÛŒ"""
    try:
        url = f"https://t.me/s/{chan}"
        res = session.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=8)
        if res.status_code == 200:
            clean_text = html.unescape(res.text)
            links = re.findall(r'(?:https://t\.me/|tg://)proxy\?(?:[^"\s>]+)', clean_text)
            return links
    except:
        pass
    return []

def run_scraper():
    """Ø§Ø¬Ø±Ø§ÛŒ Ø¹Ù…Ù„ÛŒØ§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ ØªØ³Øª"""
    print(f"\nâœ¨ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´: {datetime.now().strftime('%H:%M:%S')}")
    all_links = []
    
    with requests.Session() as session:
        with ThreadPoolExecutor(max_workers=len(CHANNELS)) as executor:
            futures = [executor.submit(fetch_channel_links, chan, session) for chan in CHANNELS]
            for future in futures:
                all_links.extend(future.result())

    unique_links = list(set(all_links))
    print(f"ğŸ“¥ ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(unique_links)}")
    
    valid_proxies = []
    with ThreadPoolExecutor(max_workers=100) as executor:
        results = executor.map(process_proxy, unique_links)
        for r in results:
            if r: valid_proxies.append(r)

    valid_proxies.sort(key=lambda x: x['ping'])

    # Ø²Ù…Ø§Ù† Ø´Ù…Ø³ÛŒ
    tehran_time = datetime.utcnow() + timedelta(hours=3, minutes=30)
    now_shamsi = jdatetime.datetime.fromgregorian(datetime=tehran_time).strftime("%Y/%m/%d - %H:%M")

    output = {"last_updated": now_shamsi, "proxies": valid_proxies}
    
    with open("proxies.json", "w", encoding="utf-8") as f:
        json.dump(output, f, indent=4, ensure_ascii=False)
    
    print(f"âœ… Ù¾Ø§ÛŒØ§Ù†. {len(valid_proxies)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

if __name__ == "__main__":
    print("ğŸš€ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ÙØ¹Ø§Ù„ Ø´Ø¯. Ù…Ù†ØªØ¸Ø± Ø§ØªØµØ§Ù„...")
    while True:
        try:
            if check_internet():
                run_scraper()
                time.sleep(RUN_INTERVAL)
            else:
                print("âš ï¸ Ø§ÛŒÙ†ØªØ±Ù†Øª Ù‚Ø·Ø¹ Ø§Ø³Øª. ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ù…Ø¬Ø¯Ø¯...")
                time.sleep(CHECK_INTERVAL)
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"ğŸ”¥ Ø®Ø·Ø§: {e}")
            time.sleep(10)
