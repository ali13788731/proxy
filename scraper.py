import requests
import re
import json
import html  # Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ù…Ø´Ú©Ù„ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ HTML Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯

# Ù„ÛŒØ³Øª Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ (Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯)
CHANNELS = [
    "Myporoxy",
    "TelMTProto",
    "ProxyMTProto",
    "mt_p_roxy",
    "ProxyHagh",
    "MTProtoProxies",
    "PinkProxy",
    "v2rayng_vpn" 
]

def scrape_channel(channel_name):
    url = f"https://t.me/s/{channel_name}"
    print(f"ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø§Ù†Ø§Ù„: {channel_name}...")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        
        # Ù…Ù‡Ù…: ØªØ¨Ø¯ÛŒÙ„ Ú©Ø¯Ù‡Ø§ÛŒ HTML Ù…Ø«Ù„ &amp; Ø¨Ù‡ Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ØµÙ„ÛŒ &
        clean_content = html.unescape(response.text)
        
        # Ø§Ù„Ú¯ÙˆÛŒ Ù…Ù†Ø¹Ø·Ùâ€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        # Ø§ÛŒÙ† Ø§Ù„Ú¯Ùˆ Ù‡Ù… Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ https Ùˆ Ù‡Ù… tg:// Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
        pattern = r'(?:https://t\.me/|tg://)proxy\?(?:[^"\s>]+)'
        
        candidates = re.findall(pattern, clean_content)
        
        valid_proxies = []
        for link in candidates:
            # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø±Ø§ Ù†Ø¯Ø§Ø±Ù†Ø¯
            if "server=" in link and "port=" in link and "secret=" in link:
                # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© Ø¨Ø§ tg Ø´Ø±ÙˆØ¹ Ø´Ø¯ØŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± ÙˆØ¨ Ø¨Ù‡ØªØ± Ø§Ø³Øª https Ø´ÙˆØ¯ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
                if link.startswith("tg://"):
                    link = link.replace("tg://", "https://t.me/")
                valid_proxies.append(link)
                
        return list(set(valid_proxies))
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± {channel_name}: {e}")
        return []

def main():
    all_proxies = []
    
    for channel in CHANNELS:
        links = scrape_channel(channel)
        print(f"   ÛŒØ§ÙØª Ø´Ø¯: {len(links)} Ø¹Ø¯Ø¯")
        
        for link in links:
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ Ø¨Ø§ Regex Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
                server_match = re.search(r'server=([\w\.\-\[\]:]+)', link)
                port_match = re.search(r'port=(\d+)', link)
                secret_match = re.search(r'secret=([\w\.\-\%]+)', link)
                
                if server_match and port_match and secret_match:
                    all_proxies.append({
                        "server": server_match.group(1),
                        "port": port_match.group(1),
                        "secret": secret_match.group(1),
                        "link": link
                    })
            except Exception as e:
                continue

    # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ±Ú©ÛŒØ¨ Ø³Ø±ÙˆØ± Ùˆ Ù¾ÙˆØ±Øª
    unique_proxies = {}
    for p in all_proxies:
        key = f"{p['server']}:{p['port']}"
        unique_proxies[key] = p
    
    final_list = list(unique_proxies.values())

    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
    with open("proxies.json", "w", encoding="utf-8") as f:
        json.dump(final_list, f, indent=4, ensure_ascii=False)
        
    print(f"\nâœ… ØªÙ…Ø§Ù… Ø´Ø¯! Ù…Ø¬Ù…ÙˆØ¹Ø§Ù‹ {len(final_list)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ù…Ù†Ø­ØµØ±â€ŒØ¨Ù‡â€ŒÙØ±Ø¯ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

if __name__ == "__main__":
    main()
